{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fases 3-5: Extracción de Características TDA y Clasificación (VERSIÓN MEJORADA)\n",
    "## Proyecto 1.b - Análisis de Grafos de Conectividad\n",
    "\n",
    "**Objetivo del Proyecto**:\n",
    "Determinar si el audio lento induce patrones de conectividad EEG diferentes comparado con el audio rápido en infantes, utilizando Análisis de Datos Topológicos (TDA) en grafos de conectividad funcional.\n",
    "\n",
    "**Hipótesis**:\n",
    "Audio lento → patrones de conectividad diferentes → firmas topológicas distintas → clasificable\n",
    "\n",
    "**Pipeline**:\n",
    "- **Fase 3**: Extraer características topológicas de matrices de distancia usando Ripser\n",
    "- **Fase 4**: Entrenar clasificadores con validación cruzada a nivel de sujeto\n",
    "- **Fase 5**: Validación estadística y prueba de hipótesis\n",
    "\n",
    "**Requisito Clave**: Separación por sujetos para evitar fuga de datos (múltiples grabaciones por sujeto)\n",
    "\n",
    "**Cambios respecto a v1**:\n",
    "- Orden de secciones corregido\n",
    "- Matriz de confusión basada en CV (no conjunto de entrenamiento)\n",
    "- Visualización de diagramas de persistencia\n",
    "- Análisis de importancia de características por banda\n",
    "- Mejor manejo de casos borde\n",
    "- Validación de datos añadida\n",
    "- Mejor documentación e interpretación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuración e Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Librerías TDA\n",
    "from ripser import ripser\n",
    "import persim\n",
    "\n",
    "# Aprendizaje Automático\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import (\n",
    "    cross_val_score,\n",
    "    cross_val_predict,\n",
    "    LeaveOneGroupOut,\n",
    "    GroupKFold,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Configurar estilo\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Pipeline de Clasificación TDA-EEG v2.0\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Librerías importadas exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuración y Rutas de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rutas\n",
    "DIRECTORIO_GRAFOS = Path(\"graphs\")\n",
    "DIRECTORIO_CARACTERISTICAS = Path(\"caracteristicas\")\n",
    "DIRECTORIO_RESULTADOS = Path(\"resultados\")\n",
    "DIRECTORIO_CARACTERISTICAS.mkdir(exist_ok=True)\n",
    "DIRECTORIO_RESULTADOS.mkdir(exist_ok=True)\n",
    "\n",
    "# Bandas de frecuencia - cada una captura diferente actividad neural\n",
    "BANDAS_FRECUENCIA = [\"delta\", \"theta\", \"alpha\", \"beta\", \"gamma\"]\n",
    "RANGOS_BANDAS = {\n",
    "    \"delta\": \"0.5-4 Hz (sueño profundo, atención)\",\n",
    "    \"theta\": \"4-8 Hz (somnolencia, memoria)\",\n",
    "    \"alpha\": \"8-13 Hz (relajación, inhibición)\",\n",
    "    \"beta\": \"13-30 Hz (alerta, pensamiento activo)\",\n",
    "    \"gamma\": \"30-100 Hz (percepción, cognición)\",\n",
    "}\n",
    "\n",
    "# Parámetros TDA\n",
    "DIMENSION_MAXIMA = 1  # Calcular H0 (componentes conexas) y H1 (ciclos/agujeros)\n",
    "# Nota: LONGITUD_ARISTA_MAXIMA debe ajustarse según tu métrica de distancia\n",
    "# Si usas distancia de correlación (1 - |corr|): rango es [0, 1], usar ~1.0\n",
    "# Si usas otras métricas, ajustar correspondientemente\n",
    "LONGITUD_ARISTA_MAXIMA = 2.0  # Valor máximo de filtración para complejo de Rips\n",
    "\n",
    "# Parámetros de clasificación\n",
    "N_PARTICIONES = 5  # Número de folds para GroupKFold CV\n",
    "N_PERMUTACIONES = 1000  # Para prueba de permutación\n",
    "N_BOOTSTRAP = 1000  # Para intervalos de confianza\n",
    "SEMILLA_ALEATORIA = 42\n",
    "\n",
    "print(\"Configuración:\")\n",
    "print(f\"  Directorio de grafos: {DIRECTORIO_GRAFOS}\")\n",
    "print(f\"  Salida de características: {DIRECTORIO_CARACTERISTICAS}\")\n",
    "print(f\"  Salida de resultados: {DIRECTORIO_RESULTADOS}\")\n",
    "print(f\"\\nBandas de frecuencia:\")\n",
    "for banda, descripcion in RANGOS_BANDAS.items():\n",
    "    print(f\"  {banda}: {descripcion}\")\n",
    "print(f\"\\nParámetros TDA:\")\n",
    "print(f\"  Dimensión homológica máxima: H{DIMENSION_MAXIMA}\")\n",
    "print(f\"  Longitud máxima de arista (filtración): {LONGITUD_ARISTA_MAXIMA}\")\n",
    "print(f\"\\nParámetros de clasificación:\")\n",
    "print(f\"  Particiones CV: {N_PARTICIONES}\")\n",
    "print(f\"  Iteraciones de permutación: {N_PERMUTACIONES}\")\n",
    "print(f\"  Iteraciones bootstrap: {N_BOOTSTRAP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 3: Análisis de Datos Topológicos\n",
    "\n",
    "## 3.1 Entendiendo el Enfoque TDA\n",
    "\n",
    "**¿Por qué TDA para conectividad EEG?**\n",
    "- Los grafos de conectividad EEG capturan relaciones funcionales entre regiones cerebrales\n",
    "- Las métricas tradicionales de grafos (densidad, clustering) pierden información estructural\n",
    "- TDA captura características topológicas multi-escala que persisten a través de valores de filtración\n",
    "\n",
    "**Qué calculamos:**\n",
    "- **H0 (componentes conexas)**: Cómo el grafo se fragmenta/conecta en diferentes umbrales\n",
    "- **H1 (ciclos/agujeros)**: Patrones circulares en conectividad que podrían indicar bucles de retroalimentación\n",
    "\n",
    "**Conexión con la hipótesis:**\n",
    "- El audio lento podría inducir actividad cerebral más sincronizada → topología de conectividad diferente\n",
    "- Estas diferencias deberían manifestarse como diagramas de persistencia diferentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Cálculo del Diagrama de Persistencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validar_matriz_distancia(matriz_distancia, nombre=\"\"):\n",
    "    \"\"\"\n",
    "    Validar que una matriz sea una matriz de distancia válida.\n",
    "    \n",
    "    Requisitos:\n",
    "    - Matriz cuadrada\n",
    "    - Simétrica (o casi simétrica)\n",
    "    - Valores no negativos\n",
    "    - Diagonal cero\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    matriz_distancia : ndarray\n",
    "        Matriz a validar\n",
    "    nombre : str\n",
    "        Nombre para mensajes de error\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "    es_valida : bool\n",
    "    problemas : lista de str\n",
    "    \"\"\"\n",
    "    problemas = []\n",
    "    \n",
    "    # Verificar cuadrada\n",
    "    if matriz_distancia.ndim != 2:\n",
    "        problemas.append(f\"No es 2D: forma={matriz_distancia.shape}\")\n",
    "        return False, problemas\n",
    "    \n",
    "    n, m = matriz_distancia.shape\n",
    "    if n != m:\n",
    "        problemas.append(f\"No es cuadrada: forma=({n}, {m})\")\n",
    "        return False, problemas\n",
    "    \n",
    "    # Verificar simetría (con tolerancia para punto flotante)\n",
    "    if not np.allclose(matriz_distancia, matriz_distancia.T, rtol=1e-5, atol=1e-8):\n",
    "        diferencia_max = np.max(np.abs(matriz_distancia - matriz_distancia.T))\n",
    "        problemas.append(f\"No es simétrica: asimetría máxima={diferencia_max:.6f}\")\n",
    "    \n",
    "    # Verificar no negativo\n",
    "    if np.any(matriz_distancia < -1e-10):  # Pequeña tolerancia para errores numéricos\n",
    "        valor_min = np.min(matriz_distancia)\n",
    "        problemas.append(f\"Valores negativos presentes: mín={valor_min:.6f}\")\n",
    "    \n",
    "    # Verificar diagonal\n",
    "    diagonal = np.diag(matriz_distancia)\n",
    "    if not np.allclose(diagonal, 0, atol=1e-10):\n",
    "        diagonal_max = np.max(np.abs(diagonal))\n",
    "        problemas.append(f\"Diagonal no cero: máx={diagonal_max:.6f}\")\n",
    "    \n",
    "    # Verificar NaN/Inf\n",
    "    if np.any(np.isnan(matriz_distancia)):\n",
    "        problemas.append(\"Contiene valores NaN\")\n",
    "    if np.any(np.isinf(matriz_distancia)):\n",
    "        problemas.append(\"Contiene valores Inf\")\n",
    "    \n",
    "    es_valida = len(problemas) == 0\n",
    "    return es_valida, problemas\n",
    "\n",
    "\n",
    "def calcular_diagrama_persistencia(matriz_distancia, dim_max=1, longitud_arista_max=2.0):\n",
    "    \"\"\"\n",
    "    Calcular diagrama de persistencia desde matriz de distancia usando Ripser.\n",
    "    \n",
    "    El diagrama de persistencia captura características topológicas (componentes, ciclos)\n",
    "    que aparecen y desaparecen al aumentar el umbral de distancia.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    matriz_distancia : ndarray, forma (n, n)\n",
    "        Matriz de distancia simétrica con diagonal cero\n",
    "    dim_max : int\n",
    "        Dimensión homológica máxima (1 = H0 y H1)\n",
    "    longitud_arista_max : float\n",
    "        Longitud máxima de arista para filtración del complejo de Rips\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "    diagramas : lista de ndarray\n",
    "        Diagramas de persistencia para cada dimensión [H0, H1, ...]\n",
    "        Cada diagrama tiene forma (n_caracteristicas, 2) con pares (nacimiento, muerte)\n",
    "    \"\"\"\n",
    "    # Asegurar que la matriz sea simétrica (promediar si hay pequeñas diferencias)\n",
    "    matriz_distancia = (matriz_distancia + matriz_distancia.T) / 2\n",
    "    np.fill_diagonal(matriz_distancia, 0)\n",
    "    \n",
    "    # Recortar valores negativos pequeños de errores numéricos\n",
    "    matriz_distancia = np.maximum(matriz_distancia, 0)\n",
    "    \n",
    "    resultado = ripser(\n",
    "        matriz_distancia,\n",
    "        maxdim=dim_max,\n",
    "        thresh=longitud_arista_max,\n",
    "        distance_matrix=True\n",
    "    )\n",
    "    return resultado[\"dgms\"]\n",
    "\n",
    "\n",
    "def extraer_caracteristicas_persistencia(diagrama, nombre_dim=\"\"):\n",
    "    \"\"\"\n",
    "    Extraer características escalares de un diagrama de persistencia.\n",
    "    \n",
    "    Las características capturan diferentes aspectos de la estructura topológica:\n",
    "    - Conteo: Cuántas características existen\n",
    "    - Tiempos de nacimiento/muerte: Cuándo aparecen/desaparecen las características\n",
    "    - Persistencia: Cuánto duran las características (muerte - nacimiento)\n",
    "    - Entropía: Distribución de valores de persistencia\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    diagrama : ndarray, forma (n_caracteristicas, 2)\n",
    "        Diagrama de persistencia con pares (nacimiento, muerte)\n",
    "    nombre_dim : str\n",
    "        Nombre de la dimensión para nombrar características\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "    caracteristicas : dict\n",
    "        Diccionario de características escalares extraídas\n",
    "    \"\"\"\n",
    "    # Remover tiempos de muerte infinitos (características esenciales que nunca mueren)\n",
    "    mascara_finita = np.isfinite(diagrama).all(axis=1)\n",
    "    diagrama_finito = diagrama[mascara_finita]\n",
    "    \n",
    "    # Contar características esenciales (aquellas con persistencia infinita)\n",
    "    n_esenciales = np.sum(~mascara_finita)\n",
    "    \n",
    "    if len(diagrama_finito) == 0:\n",
    "        # Sin características finitas - retornar ceros\n",
    "        return {\n",
    "            \"n_caracteristicas\": 0,\n",
    "            \"n_esenciales\": n_esenciales,\n",
    "            \"media_nacimiento\": 0,\n",
    "            \"std_nacimiento\": 0,\n",
    "            \"media_muerte\": 0,\n",
    "            \"std_muerte\": 0,\n",
    "            \"media_persistencia\": 0,\n",
    "            \"std_persistencia\": 0,\n",
    "            \"max_persistencia\": 0,\n",
    "            \"total_persistencia\": 0,\n",
    "            \"entropia_persistencia\": 0,\n",
    "        }\n",
    "    \n",
    "    nacimientos = diagrama_finito[:, 0]\n",
    "    muertes = diagrama_finito[:, 1]\n",
    "    persistencia = muertes - nacimientos\n",
    "    \n",
    "    # Calcular entropía de persistencia (normalizada)\n",
    "    # Mayor entropía = distribución más uniforme de valores de persistencia\n",
    "    if len(persistencia) > 1 and np.sum(persistencia) > 0:\n",
    "        p_normalizada = persistencia / np.sum(persistencia)\n",
    "        p_normalizada = p_normalizada[p_normalizada > 0]  # Remover ceros para log\n",
    "        entropia = -np.sum(p_normalizada * np.log(p_normalizada + 1e-10))\n",
    "        # Normalizar por entropía máxima posible\n",
    "        entropia = entropia / np.log(len(persistencia) + 1e-10)\n",
    "    else:\n",
    "        entropia = 0\n",
    "    \n",
    "    caracteristicas = {\n",
    "        \"n_caracteristicas\": len(diagrama_finito),\n",
    "        \"n_esenciales\": n_esenciales,\n",
    "        \"media_nacimiento\": np.mean(nacimientos),\n",
    "        \"std_nacimiento\": np.std(nacimientos) if len(nacimientos) > 1 else 0,\n",
    "        \"media_muerte\": np.mean(muertes),\n",
    "        \"std_muerte\": np.std(muertes) if len(muertes) > 1 else 0,\n",
    "        \"media_persistencia\": np.mean(persistencia),\n",
    "        \"std_persistencia\": np.std(persistencia) if len(persistencia) > 1 else 0,\n",
    "        \"max_persistencia\": np.max(persistencia),\n",
    "        \"total_persistencia\": np.sum(persistencia),\n",
    "        \"entropia_persistencia\": entropia,\n",
    "    }\n",
    "    \n",
    "    return caracteristicas\n",
    "\n",
    "\n",
    "# Probar con datos de ejemplo\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Probando Pipeline TDA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nGenerando matriz de distancia de ejemplo...\")\n",
    "np.random.seed(42)\n",
    "dist_prueba = np.random.rand(47, 47)\n",
    "dist_prueba = (dist_prueba + dist_prueba.T) / 2  # Hacer simétrica\n",
    "np.fill_diagonal(dist_prueba, 0)\n",
    "\n",
    "# Validar\n",
    "es_valida, problemas = validar_matriz_distancia(dist_prueba, \"prueba\")\n",
    "print(f\"Validación de matriz de distancia: {'PASÓ' if es_valida else 'FALLÓ'}\")\n",
    "if problemas:\n",
    "    for problema in problemas:\n",
    "        print(f\"  - {problema}\")\n",
    "\n",
    "# Calcular persistencia\n",
    "diagramas_prueba = calcular_diagrama_persistencia(dist_prueba, DIMENSION_MAXIMA, LONGITUD_ARISTA_MAXIMA)\n",
    "print(f\"\\nDiagramas de persistencia calculados:\")\n",
    "print(f\"  H0 (componentes conexas): {len(diagramas_prueba[0])} características\")\n",
    "print(f\"  H1 (ciclos/agujeros): {len(diagramas_prueba[1])} características\")\n",
    "\n",
    "# Extraer características\n",
    "caract_h0 = extraer_caracteristicas_persistencia(diagramas_prueba[0], \"H0\")\n",
    "caract_h1 = extraer_caracteristicas_persistencia(diagramas_prueba[1], \"H1\")\n",
    "print(f\"\\nCaracterísticas extraídas:\")\n",
    "print(f\"  H0: {len(caract_h0)} características escalares\")\n",
    "print(f\"  H1: {len(caract_h1)} características escalares\")\n",
    "print(f\"  Total por banda: {len(caract_h0) + len(caract_h1)} características\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Visualizar Diagramas de Persistencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graficar_diagrama_persistencia(diagramas, titulo=\"Diagrama de Persistencia\", ax=None):\n",
    "    \"\"\"\n",
    "    Graficar diagrama de persistencia con tiempos de nacimiento vs muerte.\n",
    "    \n",
    "    Puntos lejos de la diagonal = características persistentes (importantes)\n",
    "    Puntos cerca de la diagonal = ruido (características de corta vida)\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    colores = ['blue', 'orange', 'green']\n",
    "    etiquetas = ['H0 (componentes)', 'H1 (ciclos)', 'H2']\n",
    "    \n",
    "    valor_max = 0\n",
    "    for dim, dgm in enumerate(diagramas):\n",
    "        if len(dgm) > 0:\n",
    "            mascara_finita = np.isfinite(dgm).all(axis=1)\n",
    "            dgm_finito = dgm[mascara_finita]\n",
    "            \n",
    "            if len(dgm_finito) > 0:\n",
    "                ax.scatter(\n",
    "                    dgm_finito[:, 0], dgm_finito[:, 1],\n",
    "                    c=colores[dim], label=etiquetas[dim], alpha=0.6, s=50\n",
    "                )\n",
    "                valor_max = max(valor_max, dgm_finito.max())\n",
    "            \n",
    "            # Graficar características esenciales (muerte infinita) arriba\n",
    "            esenciales = dgm[~mascara_finita]\n",
    "            if len(esenciales) > 0:\n",
    "                ax.scatter(\n",
    "                    esenciales[:, 0], \n",
    "                    [valor_max * 1.1] * len(esenciales),\n",
    "                    c=colores[dim], marker='^', s=100, alpha=0.8\n",
    "                )\n",
    "    \n",
    "    # Línea diagonal (nacimiento = muerte)\n",
    "    ax.plot([0, valor_max * 1.1], [0, valor_max * 1.1], 'k--', alpha=0.3, label='Diagonal')\n",
    "    \n",
    "    ax.set_xlabel('Nacimiento', fontsize=12)\n",
    "    ax.set_ylabel('Muerte', fontsize=12)\n",
    "    ax.set_title(titulo, fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "\n",
    "# Visualizar diagrama de persistencia de prueba\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "graficar_diagrama_persistencia(diagramas_prueba, \"Diagrama de Persistencia de Ejemplo (Datos Aleatorios)\", ax)\n",
    "plt.tight_layout()\n",
    "plt.savefig(DIRECTORIO_RESULTADOS / \"diagrama_persistencia_ejemplo.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Explicación del diagrama de persistencia:\")\n",
    "print(\"  - Cada punto representa una característica topológica\")\n",
    "print(\"  - Eje X (nacimiento): umbral de distancia donde aparece la característica\")\n",
    "print(\"  - Eje Y (muerte): umbral de distancia donde desaparece la característica\")\n",
    "print(\"  - Distancia a la diagonal = persistencia = importancia\")\n",
    "print(\"  - H0: componentes conexas (cómo se fragmenta el grafo)\")\n",
    "print(\"  - H1: ciclos/agujeros (patrones de conectividad circular)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Procesar Todos los Archivos y Extraer Características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_caracteristicas_archivo(directorio_archivo, bandas_frecuencia, dim_max=1, longitud_arista_max=2.0, verbose=False):\n",
    "    \"\"\"\n",
    "    Procesar un archivo: calcular características TDA para todas las ventanas y bandas.\n",
    "    \n",
    "    Para cada banda de frecuencia:\n",
    "    1. Cargar matrices de distancia para todas las ventanas de tiempo\n",
    "    2. Calcular diagrama de persistencia para cada ventana\n",
    "    3. Extraer características escalares de cada diagrama\n",
    "    4. Agregar características a través de ventanas (media/std)\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    directorio_archivo : Path\n",
    "        Directorio que contiene matrices de distancia para un archivo\n",
    "    bandas_frecuencia : lista\n",
    "        Lista de nombres de bandas de frecuencia\n",
    "    dim_max : int\n",
    "        Dimensión homológica máxima\n",
    "    longitud_arista_max : float\n",
    "        Longitud máxima de arista para filtración de Rips\n",
    "    verbose : bool\n",
    "        Imprimir progreso detallado\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "    diccionario_caracteristicas : dict\n",
    "        Características agregadas para este archivo\n",
    "    metadatos : dict\n",
    "        Información sobre el procesamiento (n_ventanas por banda, problemas de validación)\n",
    "    \"\"\"\n",
    "    caracteristicas_archivo = {}\n",
    "    metadatos = {\"n_ventanas\": {}, \"problemas_validacion\": []}\n",
    "    \n",
    "    for banda in bandas_frecuencia:\n",
    "        archivo_dist = directorio_archivo / f\"{banda}_distances.npy\"\n",
    "        if not archivo_dist.exists():\n",
    "            if verbose:\n",
    "                print(f\"  Advertencia: {banda}_distances.npy no encontrado\")\n",
    "            metadatos[\"n_ventanas\"][banda] = 0\n",
    "            continue\n",
    "        \n",
    "        # Cargar matrices de distancia (n_ventanas, n_electrodos, n_electrodos)\n",
    "        try:\n",
    "            matrices_distancia = np.load(archivo_dist)\n",
    "        except Exception as e:\n",
    "            metadatos[\"problemas_validacion\"].append(f\"{banda}: error de carga - {e}\")\n",
    "            continue\n",
    "        \n",
    "        n_ventanas = matrices_distancia.shape[0]\n",
    "        metadatos[\"n_ventanas\"][banda] = n_ventanas\n",
    "        \n",
    "        if n_ventanas == 0:\n",
    "            if verbose:\n",
    "                print(f\"  Advertencia: {banda} tiene 0 ventanas\")\n",
    "            continue\n",
    "        \n",
    "        # Validar primera matriz\n",
    "        es_valida, problemas = validar_matriz_distancia(matrices_distancia[0], f\"{banda}[0]\")\n",
    "        if not es_valida:\n",
    "            metadatos[\"problemas_validacion\"].extend([f\"{banda}: {p}\" for p in problemas])\n",
    "            # Continuar de todos modos - intentaremos arreglar en calcular_diagrama_persistencia\n",
    "        \n",
    "        # Recolectar características de todas las ventanas\n",
    "        lista_caracteristicas_h0 = []\n",
    "        lista_caracteristicas_h1 = []\n",
    "        \n",
    "        for i in range(n_ventanas):\n",
    "            matriz_dist = matrices_distancia[i]\n",
    "            \n",
    "            try:\n",
    "                # Calcular diagramas de persistencia\n",
    "                diagramas = calcular_diagrama_persistencia(\n",
    "                    matriz_dist, dim_max, longitud_arista_max\n",
    "                )\n",
    "                \n",
    "                # Extraer características\n",
    "                caract_h0 = extraer_caracteristicas_persistencia(diagramas[0], \"H0\")\n",
    "                caract_h1 = extraer_caracteristicas_persistencia(diagramas[1], \"H1\")\n",
    "                \n",
    "                lista_caracteristicas_h0.append(caract_h0)\n",
    "                lista_caracteristicas_h1.append(caract_h1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"  Error en {banda} ventana {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Verificar si obtuvimos características válidas\n",
    "        if len(lista_caracteristicas_h0) == 0:\n",
    "            if verbose:\n",
    "                print(f\"  Advertencia: Sin ventanas válidas para {banda}\")\n",
    "            continue\n",
    "        \n",
    "        # Agregar a través de ventanas (media y std)\n",
    "        for nombre_caract in lista_caracteristicas_h0[0].keys():\n",
    "            valores_h0 = [c[nombre_caract] for c in lista_caracteristicas_h0]\n",
    "            caracteristicas_archivo[f\"{banda}_h0_{nombre_caract}_media\"] = np.mean(valores_h0)\n",
    "            caracteristicas_archivo[f\"{banda}_h0_{nombre_caract}_std\"] = np.std(valores_h0)\n",
    "            \n",
    "            valores_h1 = [c[nombre_caract] for c in lista_caracteristicas_h1]\n",
    "            caracteristicas_archivo[f\"{banda}_h1_{nombre_caract}_media\"] = np.mean(valores_h1)\n",
    "            caracteristicas_archivo[f\"{banda}_h1_{nombre_caract}_std\"] = np.std(valores_h1)\n",
    "    \n",
    "    return caracteristicas_archivo, metadatos\n",
    "\n",
    "\n",
    "# Probar en un archivo\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Probando Extracción de Características en Datos Reales\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "directorios_lento = list((DIRECTORIO_GRAFOS / \"slow\").iterdir())\n",
    "if len(directorios_lento) > 0:\n",
    "    directorio_grafo_prueba = directorios_lento[0]\n",
    "    print(f\"Probando en: {directorio_grafo_prueba.name}\")\n",
    "    \n",
    "    caracteristicas_prueba, metadatos_prueba = procesar_caracteristicas_archivo(\n",
    "        directorio_grafo_prueba, BANDAS_FRECUENCIA, DIMENSION_MAXIMA, LONGITUD_ARISTA_MAXIMA, verbose=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nResultados de extracción de características:\")\n",
    "    print(f\"  Total de características: {len(caracteristicas_prueba)}\")\n",
    "    print(f\"  Ventanas por banda: {metadatos_prueba['n_ventanas']}\")\n",
    "    if metadatos_prueba['problemas_validacion']:\n",
    "        print(f\"  Problemas de validación: {metadatos_prueba['problemas_validacion']}\")\n",
    "    print(f\"  Características de ejemplo: {list(caracteristicas_prueba.keys())[:5]}\")\n",
    "else:\n",
    "    print(\"No se encontraron datos en el directorio graphs/slow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Crear Conjunto de Datos Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_conjunto_datos(dir_grafos_lento, dir_grafos_rapido, bandas_frecuencia, dim_max=1, longitud_arista_max=2.0):\n",
    "    \"\"\"\n",
    "    Crear conjunto de datos completo desde todos los archivos.\n",
    "    \n",
    "    Extrae IDs de sujetos de los nombres de archivo para validación cruzada apropiada.\n",
    "    Formato de nombre de archivo esperado: bbXX_utYY (sujeto_ensayo)\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    X : ndarray (n_muestras, n_caracteristicas)\n",
    "    y : ndarray (n_muestras,) - 0=lento, 1=rápido\n",
    "    sujetos : ndarray (n_muestras,) - IDs de sujetos\n",
    "    nombres_caracteristicas : lista\n",
    "    nombres_archivos : lista\n",
    "    todos_metadatos : lista de dicts\n",
    "    \"\"\"\n",
    "    todas_caracteristicas = []\n",
    "    todas_etiquetas = []\n",
    "    todos_sujetos = []\n",
    "    todos_nombres_archivos = []\n",
    "    todos_metadatos = []\n",
    "    \n",
    "    # Procesar archivos lentos (etiqueta = 0)\n",
    "    directorios_lento = sorted([d for d in dir_grafos_lento.iterdir() if d.is_dir()])\n",
    "    print(f\"\\nProcesando {len(directorios_lento)} archivos de audio LENTO...\")\n",
    "    \n",
    "    for directorio_archivo in tqdm(directorios_lento, desc=\"Lento\"):\n",
    "        try:\n",
    "            caracteristicas, metadatos = procesar_caracteristicas_archivo(\n",
    "                directorio_archivo, bandas_frecuencia, dim_max, longitud_arista_max\n",
    "            )\n",
    "            if len(caracteristicas) > 0:\n",
    "                todas_caracteristicas.append(caracteristicas)\n",
    "                todas_etiquetas.append(0)  # lento = 0\n",
    "                \n",
    "                # Extraer ID de sujeto (formato: bbXX_utYY)\n",
    "                nombre_archivo = directorio_archivo.name\n",
    "                partes = nombre_archivo.split(\"_\")\n",
    "                id_sujeto = partes[0] if len(partes) > 0 else nombre_archivo\n",
    "                \n",
    "                todos_sujetos.append(id_sujeto)\n",
    "                todos_nombres_archivos.append(nombre_archivo)\n",
    "                todos_metadatos.append(metadatos)\n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando {directorio_archivo.name}: {e}\")\n",
    "    \n",
    "    # Procesar archivos rápidos (etiqueta = 1)\n",
    "    directorios_rapido = sorted([d for d in dir_grafos_rapido.iterdir() if d.is_dir()])\n",
    "    print(f\"Procesando {len(directorios_rapido)} archivos de audio RÁPIDO...\")\n",
    "    \n",
    "    for directorio_archivo in tqdm(directorios_rapido, desc=\"Rápido\"):\n",
    "        try:\n",
    "            caracteristicas, metadatos = procesar_caracteristicas_archivo(\n",
    "                directorio_archivo, bandas_frecuencia, dim_max, longitud_arista_max\n",
    "            )\n",
    "            if len(caracteristicas) > 0:\n",
    "                todas_caracteristicas.append(caracteristicas)\n",
    "                todas_etiquetas.append(1)  # rápido = 1\n",
    "                \n",
    "                nombre_archivo = directorio_archivo.name\n",
    "                partes = nombre_archivo.split(\"_\")\n",
    "                id_sujeto = partes[0] if len(partes) > 0 else nombre_archivo\n",
    "                \n",
    "                todos_sujetos.append(id_sujeto)\n",
    "                todos_nombres_archivos.append(nombre_archivo)\n",
    "                todos_metadatos.append(metadatos)\n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando {directorio_archivo.name}: {e}\")\n",
    "    \n",
    "    # Convertir a arrays\n",
    "    df_caracteristicas = pd.DataFrame(todas_caracteristicas)\n",
    "    nombres_caracteristicas = list(df_caracteristicas.columns)\n",
    "    X = df_caracteristicas.values\n",
    "    y = np.array(todas_etiquetas)\n",
    "    sujetos = np.array(todos_sujetos)\n",
    "    \n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"Resumen del Conjunto de Datos\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total de muestras: {X.shape[0]}\")\n",
    "    print(f\"Total de características: {X.shape[1]}\")\n",
    "    print(f\"  Características por banda: {X.shape[1] // len(bandas_frecuencia)}\")\n",
    "    print(f\"\\nDistribución de clases:\")\n",
    "    print(f\"  Lento (0): {np.sum(y == 0)} muestras ({np.sum(y == 0) / len(y) * 100:.1f}%)\")\n",
    "    print(f\"  Rápido (1): {np.sum(y == 1)} muestras ({np.sum(y == 1) / len(y) * 100:.1f}%)\")\n",
    "    print(f\"\\nDistribución de sujetos:\")\n",
    "    print(f\"  Sujetos únicos: {len(np.unique(sujetos))}\")\n",
    "    \n",
    "    return X, y, sujetos, nombres_caracteristicas, todos_nombres_archivos, todos_metadatos\n",
    "\n",
    "\n",
    "# Crear conjunto de datos\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Creando Conjunto de Datos Completo\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "X, y, sujetos, nombres_caracteristicas, nombres_archivos, todos_metadatos = crear_conjunto_datos(\n",
    "    DIRECTORIO_GRAFOS / \"slow\",\n",
    "    DIRECTORIO_GRAFOS / \"fast\",\n",
    "    BANDAS_FRECUENCIA,\n",
    "    DIMENSION_MAXIMA,\n",
    "    LONGITUD_ARISTA_MAXIMA\n",
    ")\n",
    "\n",
    "# Guardar conjunto de datos\n",
    "np.save(DIRECTORIO_CARACTERISTICAS / \"X.npy\", X)\n",
    "np.save(DIRECTORIO_CARACTERISTICAS / \"y.npy\", y)\n",
    "np.save(DIRECTORIO_CARACTERISTICAS / \"sujetos.npy\", sujetos)\n",
    "\n",
    "with open(DIRECTORIO_CARACTERISTICAS / \"nombres_caracteristicas.txt\", \"w\") as f:\n",
    "    for nombre in nombres_caracteristicas:\n",
    "        f.write(f\"{nombre}\\n\")\n",
    "\n",
    "with open(DIRECTORIO_CARACTERISTICAS / \"nombres_archivos.txt\", \"w\") as f:\n",
    "    for nombre in nombres_archivos:\n",
    "        f.write(f\"{nombre}\\n\")\n",
    "\n",
    "print(f\"\\nConjunto de datos guardado en {DIRECTORIO_CARACTERISTICAS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Preprocesamiento y Validación de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Preprocesamiento de Datos\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verificar valores NaN/Inf\n",
    "print(\"\\nVerificando valores inválidos...\")\n",
    "conteo_nan = np.isnan(X).sum()\n",
    "conteo_inf = np.isinf(X).sum()\n",
    "print(f\"  Valores NaN: {conteo_nan}\")\n",
    "print(f\"  Valores Inf: {conteo_inf}\")\n",
    "\n",
    "# Manejar valores problemáticos\n",
    "mascara_nan = np.isnan(X).any(axis=1)\n",
    "mascara_inf = np.isinf(X).any(axis=1)\n",
    "mascara_valida = ~(mascara_nan | mascara_inf)\n",
    "\n",
    "n_eliminados = (~mascara_valida).sum()\n",
    "if n_eliminados > 0:\n",
    "    print(f\"\\nEliminando {n_eliminados} muestras con valores inválidos...\")\n",
    "    X = X[mascara_valida]\n",
    "    y = y[mascara_valida]\n",
    "    sujetos = sujetos[mascara_valida]\n",
    "    nombres_archivos = [f for f, v in zip(nombres_archivos, mascara_valida) if v]\n",
    "\n",
    "print(f\"\\nConjunto de datos limpio: {X.shape[0]} muestras, {X.shape[1]} características\")\n",
    "\n",
    "# Analizar distribuciones de características\n",
    "print(\"\\nEstadísticas de características:\")\n",
    "print(f\"  Valor mínimo: {X.min():.4f}\")\n",
    "print(f\"  Valor máximo: {X.max():.4f}\")\n",
    "print(f\"  Media: {X.mean():.4f}\")\n",
    "print(f\"  Desviación estándar: {X.std():.4f}\")\n",
    "\n",
    "# Verificar características constantes (varianza cero)\n",
    "stds_caracteristicas = X.std(axis=0)\n",
    "caracteristicas_constantes = stds_caracteristicas < 1e-10\n",
    "n_constantes = caracteristicas_constantes.sum()\n",
    "if n_constantes > 0:\n",
    "    print(f\"\\nAdvertencia: {n_constantes} características tienen varianza cero\")\n",
    "    print(\"  Estas no serán informativas para la clasificación\")\n",
    "    nombres_constantes = [nombres_caracteristicas[i] for i in np.where(caracteristicas_constantes)[0]]\n",
    "    print(f\"  Características constantes: {nombres_constantes[:5]}...\")\n",
    "\n",
    "# Estandarizar características\n",
    "escalador = StandardScaler()\n",
    "X_escalado = escalador.fit_transform(X)\n",
    "\n",
    "print(\"\\nCaracterísticas estandarizadas (media=0, std=1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Análisis de Distribución de Sujetos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Análisis de Distribución de Sujetos\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Crear resumen a nivel de sujeto\n",
    "df_sujetos = pd.DataFrame({\n",
    "    \"sujeto\": sujetos,\n",
    "    \"etiqueta\": y,\n",
    "    \"nombre_etiqueta\": [\"lento\" if e == 0 else \"rapido\" for e in y]\n",
    "})\n",
    "\n",
    "# Muestras por sujeto\n",
    "conteo_sujetos = df_sujetos.groupby(\"sujeto\").size()\n",
    "print(f\"\\nMuestras por sujeto:\")\n",
    "print(f\"  Media: {conteo_sujetos.mean():.1f}\")\n",
    "print(f\"  Mediana: {conteo_sujetos.median():.1f}\")\n",
    "print(f\"  Mínimo: {conteo_sujetos.min()}\")\n",
    "print(f\"  Máximo: {conteo_sujetos.max()}\")\n",
    "\n",
    "# Distribución de etiquetas por sujeto\n",
    "etiquetas_sujeto = df_sujetos.groupby(\"sujeto\")[\"etiqueta\"].agg([\"count\", \"sum\", \"mean\"])\n",
    "etiquetas_sujeto.columns = [\"total\", \"n_rapido\", \"prop_rapido\"]\n",
    "etiquetas_sujeto[\"n_lento\"] = etiquetas_sujeto[\"total\"] - etiquetas_sujeto[\"n_rapido\"]\n",
    "\n",
    "print(f\"\\nDistribución de etiquetas por sujeto:\")\n",
    "print(etiquetas_sujeto.describe())\n",
    "\n",
    "# Verificar si los sujetos tienen ambas condiciones (diseño intra-sujeto)\n",
    "sujetos_mixtos = etiquetas_sujeto[(etiquetas_sujeto[\"n_lento\"] > 0) & (etiquetas_sujeto[\"n_rapido\"] > 0)]\n",
    "print(f\"\\nSujetos con ambas condiciones: {len(sujetos_mixtos)} / {len(etiquetas_sujeto)}\")\n",
    "\n",
    "# Visualizar\n",
    "fig, ejes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Gráfico 1: Muestras por sujeto\n",
    "ax1 = ejes[0]\n",
    "conteo_sujetos.plot(kind=\"bar\", ax=ax1, color=\"steelblue\", alpha=0.7)\n",
    "ax1.set_xlabel(\"ID de Sujeto\")\n",
    "ax1.set_ylabel(\"Número de Muestras\")\n",
    "ax1.set_title(\"Muestras por Sujeto\", fontweight=\"bold\")\n",
    "ax1.axhline(conteo_sujetos.mean(), color=\"red\", linestyle=\"--\", label=f\"Media: {conteo_sujetos.mean():.1f}\")\n",
    "ax1.legend()\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Gráfico 2: Distribución de etiquetas\n",
    "ax2 = ejes[1]\n",
    "etiquetas_sujeto[[\"n_lento\", \"n_rapido\"]].plot(kind=\"bar\", stacked=True, ax=ax2, color=[\"blue\", \"orange\"], alpha=0.7)\n",
    "ax2.set_xlabel(\"ID de Sujeto\")\n",
    "ax2.set_ylabel(\"Número de Muestras\")\n",
    "ax2.set_title(\"Distribución de Clases por Sujeto\", fontweight=\"bold\")\n",
    "ax2.legend([\"Lento\", \"Rápido\"])\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(DIRECTORIO_RESULTADOS / \"distribucion_sujetos.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 4: Clasificación\n",
    "\n",
    "## 8. Estrategia de Validación Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Estrategia de Validación Cruzada\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\"\"\n",
    "Por qué la CV a nivel de sujeto es crítica:\n",
    "--------------------------------------------\n",
    "1. Múltiples grabaciones por sujeto comparten patrones específicos del sujeto\n",
    "2. Si el mismo sujeto aparece en entrenamiento y prueba, el modelo aprende identidad del sujeto\n",
    "3. Esto causa fuga de datos y precisión inflada\n",
    "4. La CV a nivel de sujeto asegura generalización a NUEVOS sujetos\n",
    "\n",
    "Estrategia: GroupKFold (n_particiones={N_PARTICIONES})\n",
    "- Grupos = sujetos\n",
    "- Cada fold: ~{100 // N_PARTICIONES}% de sujetos en conjunto de prueba\n",
    "- Ningún sujeto aparece en entrenamiento y prueba simultáneamente\n",
    "- Más robusto que Leave-One-Subject-Out (conjuntos de prueba más grandes)\n",
    "\"\"\")\n",
    "\n",
    "# Configurar CV\n",
    "gkf = GroupKFold(n_splits=N_PARTICIONES)\n",
    "\n",
    "# Verificar particiones de CV\n",
    "print(\"Verificando particiones de CV...\")\n",
    "for fold, (idx_entrenamiento, idx_prueba) in enumerate(gkf.split(X_escalado, y, groups=sujetos)):\n",
    "    sujetos_entrenamiento = set(sujetos[idx_entrenamiento])\n",
    "    sujetos_prueba = set(sujetos[idx_prueba])\n",
    "    solapamiento = sujetos_entrenamiento.intersection(sujetos_prueba)\n",
    "    \n",
    "    print(f\"  Fold {fold + 1}:\")\n",
    "    print(f\"    Entrenamiento: {len(idx_entrenamiento)} muestras, {len(sujetos_entrenamiento)} sujetos\")\n",
    "    print(f\"    Prueba: {len(idx_prueba)} muestras, {len(sujetos_prueba)} sujetos\")\n",
    "    print(f\"    Solapamiento de sujetos: {len(solapamiento)} (debería ser 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Entrenamiento y Evaluación del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Entrenamiento y Evaluación del Modelo\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Inicializar modelo\n",
    "modelo_rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=SEMILLA_ALEATORIA,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Puntajes de validación cruzada\n",
    "print(f\"\\nEntrenando Random Forest con GroupKFold CV (k={N_PARTICIONES})...\")\n",
    "puntajes_cv = cross_val_score(\n",
    "    modelo_rf, X_escalado, y, groups=sujetos, cv=gkf, scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "print(f\"\\nResultados de Validación Cruzada:\")\n",
    "print(f\"  Precisión por fold: {puntajes_cv}\")\n",
    "print(f\"  Precisión media: {puntajes_cv.mean():.3f} +/- {puntajes_cv.std():.3f}\")\n",
    "print(f\"  Mín/Máx: {puntajes_cv.min():.3f} / {puntajes_cv.max():.3f}\")\n",
    "\n",
    "# Obtener predicciones de CV para matriz de confusión (¡IMPORTANTE: no predicciones de entrenamiento!)\n",
    "print(\"\\nObteniendo predicciones validadas cruzadamente...\")\n",
    "y_pred_cv = cross_val_predict(modelo_rf, X_escalado, y, groups=sujetos, cv=gkf)\n",
    "\n",
    "# Calcular métricas adicionales\n",
    "f1_cv = f1_score(y, y_pred_cv, average=\"weighted\")\n",
    "print(f\"  Puntaje F1 (ponderado): {f1_cv:.3f}\")\n",
    "\n",
    "# Intentar calcular AUC si es posible\n",
    "try:\n",
    "    # Obtener predicciones de probabilidad\n",
    "    y_proba_cv = cross_val_predict(\n",
    "        modelo_rf, X_escalado, y, groups=sujetos, cv=gkf, method=\"predict_proba\"\n",
    "    )\n",
    "    auc_cv = roc_auc_score(y, y_proba_cv[:, 1])\n",
    "    print(f\"  ROC AUC: {auc_cv:.3f}\")\n",
    "except Exception as e:\n",
    "    print(f\"  ROC AUC: No se pudo calcular ({e})\")\n",
    "    auc_cv = None\n",
    "\n",
    "# Reporte de clasificación (en predicciones de CV)\n",
    "print(\"\\nReporte de Clasificación (Validación Cruzada):\")\n",
    "print(classification_report(y, y_pred_cv, target_names=[\"Lento\", \"Rápido\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Matriz de Confusión (Validación Cruzada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¡IMPORTANTE: Usando predicciones de CV, no predicciones de entrenamiento!\n",
    "mc = confusion_matrix(y, y_pred_cv)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    mc,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=[\"Lento\", \"Rápido\"],\n",
    "    yticklabels=[\"Lento\", \"Rápido\"],\n",
    "    ax=ax,\n",
    "    annot_kws={\"size\": 16}\n",
    ")\n",
    "ax.set_xlabel(\"Predicción\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Real\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_title(\"Matriz de Confusión (Predicciones de Validación Cruzada)\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "# Añadir texto con métricas\n",
    "texto_metricas = f\"Precisión CV: {puntajes_cv.mean():.1%}\\nPuntaje F1: {f1_cv:.3f}\"\n",
    "props = dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.8)\n",
    "ax.text(1.35, 0.5, texto_metricas, transform=ax.transAxes, fontsize=12,\n",
    "        verticalalignment=\"center\", bbox=props)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(DIRECTORIO_RESULTADOS / \"matriz_confusion_cv.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Nota: Esta matriz de confusión usa predicciones de VALIDACIÓN CRUZADA,\")\n",
    "print(\"no predicciones del conjunto de entrenamiento. Esto da una estimación honesta del rendimiento.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Análisis de Importancia de Características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Análisis de Importancia de Características\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Entrenar en conjunto completo para importancia de características\n",
    "modelo_rf.fit(X_escalado, y)\n",
    "importancia_caracteristicas = modelo_rf.feature_importances_\n",
    "\n",
    "# Crear DataFrame de importancia de características\n",
    "df_importancia = pd.DataFrame({\n",
    "    \"caracteristica\": nombres_caracteristicas,\n",
    "    \"importancia\": importancia_caracteristicas\n",
    "}).sort_values(\"importancia\", ascending=False)\n",
    "\n",
    "# Extraer información de banda y dimensión\n",
    "df_importancia[\"banda\"] = df_importancia[\"caracteristica\"].apply(\n",
    "    lambda x: x.split(\"_\")[0] if \"_\" in x else \"desconocido\"\n",
    ")\n",
    "df_importancia[\"dimension\"] = df_importancia[\"caracteristica\"].apply(\n",
    "    lambda x: \"H0\" if \"_h0_\" in x else \"H1\" if \"_h1_\" in x else \"desconocido\"\n",
    ")\n",
    "\n",
    "# Top 15 características\n",
    "print(\"\\nTop 15 Características Más Importantes:\")\n",
    "print(\"-\" * 60)\n",
    "for i, fila in df_importancia.head(15).iterrows():\n",
    "    print(f\"  {fila['caracteristica']}: {fila['importancia']:.4f}\")\n",
    "\n",
    "# Visualizar características principales\n",
    "fig, ejes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Gráfico 1: Top 15 características\n",
    "ax1 = ejes[0]\n",
    "top_15 = df_importancia.head(15)\n",
    "colores = [\"blue\" if \"h0\" in c else \"orange\" for c in top_15[\"caracteristica\"]]\n",
    "ax1.barh(range(15), top_15[\"importancia\"].values, color=colores, alpha=0.7)\n",
    "ax1.set_yticks(range(15))\n",
    "ax1.set_yticklabels(top_15[\"caracteristica\"].values, fontsize=9)\n",
    "ax1.set_xlabel(\"Importancia\", fontsize=12)\n",
    "ax1.set_title(\"Top 15 Características Más Importantes\", fontsize=14, fontweight=\"bold\")\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# Añadir leyenda\n",
    "from matplotlib.patches import Patch\n",
    "elementos_leyenda = [Patch(facecolor=\"blue\", alpha=0.7, label=\"H0 (componentes)\"),\n",
    "                     Patch(facecolor=\"orange\", alpha=0.7, label=\"H1 (ciclos)\")]\n",
    "ax1.legend(handles=elementos_leyenda, loc=\"lower right\")\n",
    "\n",
    "# Gráfico 2: Importancia por banda de frecuencia\n",
    "ax2 = ejes[1]\n",
    "importancia_banda = df_importancia.groupby(\"banda\")[\"importancia\"].sum().sort_values(ascending=True)\n",
    "colores_banda = plt.cm.viridis(np.linspace(0.2, 0.8, len(importancia_banda)))\n",
    "ax2.barh(importancia_banda.index, importancia_banda.values, color=colores_banda)\n",
    "ax2.set_xlabel(\"Importancia Total\", fontsize=12)\n",
    "ax2.set_title(\"Importancia por Banda de Frecuencia\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "# Añadir etiquetas de porcentaje\n",
    "importancia_total = importancia_banda.sum()\n",
    "for i, (banda, imp) in enumerate(importancia_banda.items()):\n",
    "    ax2.text(imp + 0.01, i, f\"{imp / importancia_total * 100:.1f}%\", va=\"center\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(DIRECTORIO_RESULTADOS / \"importancia_caracteristicas.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Resumen por banda y dimensión\n",
    "print(\"\\nResumen de Importancia por Banda:\")\n",
    "resumen_banda = df_importancia.groupby(\"banda\")[\"importancia\"].agg([\"sum\", \"mean\"]).sort_values(\"sum\", ascending=False)\n",
    "resumen_banda[\"porcentaje\"] = resumen_banda[\"sum\"] / resumen_banda[\"sum\"].sum() * 100\n",
    "print(resumen_banda.round(4))\n",
    "\n",
    "print(\"\\nResumen de Importancia por Dimensión:\")\n",
    "resumen_dim = df_importancia.groupby(\"dimension\")[\"importancia\"].agg([\"sum\", \"mean\"])\n",
    "resumen_dim[\"porcentaje\"] = resumen_dim[\"sum\"] / resumen_dim[\"sum\"].sum() * 100\n",
    "print(resumen_dim.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 5: Validación Estadística\n",
    "\n",
    "## 12. Prueba de Permutación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Validación Estadística: Prueba de Permutación\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "Prueba de Permutación:\n",
    "----------------------\n",
    "H0 (hipótesis nula): La precisión de clasificación se debe al azar\n",
    "H1 (alternativa): Las características topológicas contienen información discriminativa\n",
    "\n",
    "Método: Permutar etiquetas N veces, calcular precisión CV cada vez\n",
    "Valor p: Proporción de precisiones permutadas >= precisión observada\n",
    "\"\"\")\n",
    "\n",
    "def prueba_permutacion_cv(X, y, grupos, modelo, cv, n_permutaciones=1000, semilla=42):\n",
    "    \"\"\"Prueba de permutación para precisión de validación cruzada.\"\"\"\n",
    "    # Puntaje observado\n",
    "    puntajes_observados = cross_val_score(modelo, X, y, groups=grupos, cv=cv, scoring=\"accuracy\")\n",
    "    media_observada = puntajes_observados.mean()\n",
    "    \n",
    "    # Distribución nula\n",
    "    np.random.seed(semilla)\n",
    "    distribucion_nula = []\n",
    "    \n",
    "    for i in tqdm(range(n_permutaciones), desc=\"Prueba de permutación\"):\n",
    "        y_permutado = np.random.permutation(y)\n",
    "        puntajes_perm = cross_val_score(modelo, X, y_permutado, groups=grupos, cv=cv, scoring=\"accuracy\")\n",
    "        distribucion_nula.append(puntajes_perm.mean())\n",
    "    \n",
    "    distribucion_nula = np.array(distribucion_nula)\n",
    "    \n",
    "    # Valor p (una cola, probando si observado > nulo)\n",
    "    valor_p = (np.sum(distribucion_nula >= media_observada) + 1) / (n_permutaciones + 1)\n",
    "    \n",
    "    # Tamaño del efecto (d de Cohen)\n",
    "    tamano_efecto = (media_observada - distribucion_nula.mean()) / distribucion_nula.std()\n",
    "    \n",
    "    return media_observada, distribucion_nula, valor_p, tamano_efecto\n",
    "\n",
    "\n",
    "# Ejecutar prueba de permutación\n",
    "precision_observada, dist_nula, valor_p, tamano_efecto = prueba_permutacion_cv(\n",
    "    X_escalado, y, sujetos, modelo_rf, gkf, \n",
    "    n_permutaciones=N_PERMUTACIONES, \n",
    "    semilla=SEMILLA_ALEATORIA\n",
    ")\n",
    "\n",
    "print(f\"\\nResultados de Prueba de Permutación:\")\n",
    "print(f\"  Precisión CV observada: {precision_observada:.4f} ({precision_observada:.1%})\")\n",
    "print(f\"  Media distribución nula: {dist_nula.mean():.4f} ({dist_nula.mean():.1%})\")\n",
    "print(f\"  Std distribución nula: {dist_nula.std():.4f}\")\n",
    "print(f\"  Valor p: {valor_p:.6f}\")\n",
    "print(f\"  Tamaño del efecto (d de Cohen): {tamano_efecto:.2f}\")\n",
    "\n",
    "# Interpretar tamaño del efecto\n",
    "if abs(tamano_efecto) < 0.2:\n",
    "    interpretacion_efecto = \"insignificante\"\n",
    "elif abs(tamano_efecto) < 0.5:\n",
    "    interpretacion_efecto = \"pequeño\"\n",
    "elif abs(tamano_efecto) < 0.8:\n",
    "    interpretacion_efecto = \"mediano\"\n",
    "else:\n",
    "    interpretacion_efecto = \"grande\"\n",
    "print(f\"  Interpretación del efecto: {interpretacion_efecto}\")\n",
    "\n",
    "# Interpretar significancia\n",
    "alfa = 0.05\n",
    "if valor_p < 0.001:\n",
    "    nivel_sig = \"*** (p < 0.001)\"\n",
    "elif valor_p < 0.01:\n",
    "    nivel_sig = \"** (p < 0.01)\"\n",
    "elif valor_p < 0.05:\n",
    "    nivel_sig = \"* (p < 0.05)\"\n",
    "else:\n",
    "    nivel_sig = \"ns (p >= 0.05)\"\n",
    "print(f\"  Significancia: {nivel_sig}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Intervalo de Confianza Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Validación Estadística: Intervalo de Confianza Bootstrap\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def bootstrap_puntaje_cv(X, y, grupos, modelo, cv, n_bootstrap=1000, semilla=42):\n",
    "    \"\"\"Intervalo de confianza bootstrap para precisión CV mediante remuestreo de sujetos.\"\"\"\n",
    "    np.random.seed(semilla)\n",
    "    puntajes_bootstrap = []\n",
    "    \n",
    "    sujetos_unicos = np.unique(grupos)\n",
    "    n_sujetos = len(sujetos_unicos)\n",
    "    \n",
    "    for i in tqdm(range(n_bootstrap), desc=\"Bootstrap\"):\n",
    "        # Remuestrear sujetos con reemplazo\n",
    "        sujetos_boot = np.random.choice(sujetos_unicos, size=n_sujetos, replace=True)\n",
    "        \n",
    "        # Obtener índices para sujetos seleccionados\n",
    "        indices_boot = []\n",
    "        nuevos_grupos = []\n",
    "        for j, sujeto in enumerate(sujetos_boot):\n",
    "            indices_sujeto = np.where(grupos == sujeto)[0]\n",
    "            indices_boot.extend(indices_sujeto)\n",
    "            # Asignar nuevo ID de grupo para manejar sujetos duplicados\n",
    "            nuevos_grupos.extend([j] * len(indices_sujeto))\n",
    "        \n",
    "        indices_boot = np.array(indices_boot)\n",
    "        nuevos_grupos = np.array(nuevos_grupos)\n",
    "        \n",
    "        X_boot = X[indices_boot]\n",
    "        y_boot = y[indices_boot]\n",
    "        \n",
    "        # Verificar que tenemos ambas clases y suficientes grupos\n",
    "        if len(np.unique(y_boot)) < 2:\n",
    "            continue\n",
    "        if len(np.unique(nuevos_grupos)) < cv.n_splits:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            puntajes_boot = cross_val_score(\n",
    "                modelo, X_boot, y_boot, groups=nuevos_grupos, cv=cv, scoring=\"accuracy\"\n",
    "            )\n",
    "            puntajes_bootstrap.append(puntajes_boot.mean())\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    return np.array(puntajes_bootstrap)\n",
    "\n",
    "\n",
    "# Ejecutar bootstrap\n",
    "precisiones_bootstrap = bootstrap_puntaje_cv(\n",
    "    X_escalado, y, sujetos, modelo_rf, gkf,\n",
    "    n_bootstrap=N_BOOTSTRAP,\n",
    "    semilla=SEMILLA_ALEATORIA\n",
    ")\n",
    "\n",
    "# Intervalo de confianza (método de percentiles)\n",
    "ic_inferior = np.percentile(precisiones_bootstrap, 2.5)\n",
    "ic_superior = np.percentile(precisiones_bootstrap, 97.5)\n",
    "ancho_ic = ic_superior - ic_inferior\n",
    "\n",
    "print(f\"\\nResultados Bootstrap ({len(precisiones_bootstrap)} iteraciones exitosas):\")\n",
    "print(f\"  Precisión media: {precisiones_bootstrap.mean():.4f} ({precisiones_bootstrap.mean():.1%})\")\n",
    "print(f\"  Std: {precisiones_bootstrap.std():.4f}\")\n",
    "print(f\"  Mediana: {np.median(precisiones_bootstrap):.4f}\")\n",
    "print(f\"\\nIntervalo de Confianza 95%:\")\n",
    "print(f\"  [{ic_inferior:.4f}, {ic_superior:.4f}]\")\n",
    "print(f\"  [{ic_inferior:.1%}, {ic_superior:.1%}]\")\n",
    "print(f\"  Ancho: {ancho_ic:.4f} ({ancho_ic:.1%})\")\n",
    "\n",
    "# Verificar si IC excluye el azar\n",
    "if ic_inferior > 0.5:\n",
    "    print(f\"\\n  ✓ El IC completo está sobre el azar (50%) - resultado robusto\")\n",
    "else:\n",
    "    print(f\"\\n  ⚠ El IC incluye el nivel de azar (50%) - el resultado puede no ser robusto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Visualización de Pruebas Estadísticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ejes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Gráfico 1: Prueba de Permutación\n",
    "ax1 = ejes[0]\n",
    "ax1.hist(dist_nula, bins=50, alpha=0.7, color=\"gray\", edgecolor=\"black\", density=True,\n",
    "         label=f\"Distribución nula (n={N_PERMUTACIONES})\")\n",
    "ax1.axvline(precision_observada, color=\"red\", linewidth=3, linestyle=\"--\",\n",
    "            label=f\"Observada ({precision_observada:.1%})\")\n",
    "ax1.axvline(dist_nula.mean(), color=\"blue\", linewidth=2, linestyle=\":\",\n",
    "            label=f\"Media nula ({dist_nula.mean():.1%})\")\n",
    "ax1.axvline(0.5, color=\"green\", linewidth=2, linestyle=\"-.\",\n",
    "            label=\"Azar (50%)\")\n",
    "\n",
    "ax1.set_xlabel(\"Precisión de Validación Cruzada\", fontsize=12, fontweight=\"bold\")\n",
    "ax1.set_ylabel(\"Densidad\", fontsize=12, fontweight=\"bold\")\n",
    "ax1.set_title(\"Prueba de Permutación: Distribución Nula\", fontsize=14, fontweight=\"bold\")\n",
    "ax1.legend(loc=\"upper left\", fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Añadir anotación de valor p\n",
    "texto_p = f\"p = {valor_p:.4f}\\nd de Cohen = {tamano_efecto:.2f}\\n{nivel_sig}\"\n",
    "props = dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.9)\n",
    "ax1.text(0.97, 0.97, texto_p, transform=ax1.transAxes, fontsize=11,\n",
    "         verticalalignment=\"top\", horizontalalignment=\"right\", bbox=props)\n",
    "\n",
    "# Gráfico 2: Distribución Bootstrap\n",
    "ax2 = ejes[1]\n",
    "ax2.hist(precisiones_bootstrap, bins=50, alpha=0.7, color=\"steelblue\", edgecolor=\"black\", density=True,\n",
    "         label=f\"Distribución bootstrap (n={len(precisiones_bootstrap)})\")\n",
    "ax2.axvline(precision_observada, color=\"red\", linewidth=3, linestyle=\"--\",\n",
    "            label=f\"Observada ({precision_observada:.1%})\")\n",
    "ax2.axvline(ic_inferior, color=\"orange\", linewidth=2, linestyle=\":\",\n",
    "            label=f\"IC 95%: [{ic_inferior:.1%}, {ic_superior:.1%}]\")\n",
    "ax2.axvline(ic_superior, color=\"orange\", linewidth=2, linestyle=\":\")\n",
    "ax2.axvspan(ic_inferior, ic_superior, alpha=0.2, color=\"orange\")\n",
    "ax2.axvline(0.5, color=\"green\", linewidth=2, linestyle=\"-.\", label=\"Azar (50%)\")\n",
    "\n",
    "ax2.set_xlabel(\"Precisión de Validación Cruzada\", fontsize=12, fontweight=\"bold\")\n",
    "ax2.set_ylabel(\"Densidad\", fontsize=12, fontweight=\"bold\")\n",
    "ax2.set_title(\"Bootstrap: Intervalo de Confianza 95%\", fontsize=14, fontweight=\"bold\")\n",
    "ax2.legend(loc=\"upper left\", fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Añadir anotación de IC\n",
    "texto_ic = f\"IC 95%:\\n[{ic_inferior:.1%}, {ic_superior:.1%}]\\nAncho: {ancho_ic:.1%}\"\n",
    "props = dict(boxstyle=\"round\", facecolor=\"lightblue\", alpha=0.9)\n",
    "ax2.text(0.97, 0.97, texto_ic, transform=ax2.transAxes, fontsize=11,\n",
    "         verticalalignment=\"top\", horizontalalignment=\"right\", bbox=props)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(DIRECTORIO_RESULTADOS / \"pruebas_estadisticas.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Resultados Finales y Conclusiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RESULTADOS FINALES Y CONCLUSIONES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Tabla resumen\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"TABLA RESUMEN\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "resumen_resultados = {\n",
    "    \"Métrica\": [\n",
    "        \"Tamaño del conjunto de datos\",\n",
    "        \"Número de características\",\n",
    "        \"Número de sujetos\",\n",
    "        \"Balance de clases (Lento/Rápido)\",\n",
    "        \"\",\n",
    "        \"Precisión CV (GroupKFold-5)\",\n",
    "        \"Puntaje F1 (ponderado)\",\n",
    "        \"Línea base (azar)\",\n",
    "        \"Mejora sobre línea base\",\n",
    "        \"\",\n",
    "        \"Valor p (prueba de permutación)\",\n",
    "        \"Tamaño del efecto (d de Cohen)\",\n",
    "        \"Límite inferior IC 95%\",\n",
    "        \"Límite superior IC 95%\",\n",
    "        \"¿IC sobre el azar?\",\n",
    "    ],\n",
    "    \"Valor\": [\n",
    "        f\"{X.shape[0]} muestras\",\n",
    "        f\"{X.shape[1]} características\",\n",
    "        f\"{len(np.unique(sujetos))} sujetos\",\n",
    "        f\"{np.sum(y == 0)} / {np.sum(y == 1)}\",\n",
    "        \"\",\n",
    "        f\"{puntajes_cv.mean():.1%} ± {puntajes_cv.std():.1%}\",\n",
    "        f\"{f1_cv:.3f}\",\n",
    "        \"50%\",\n",
    "        f\"+{(puntajes_cv.mean() - 0.5) * 100:.1f} puntos porcentuales\",\n",
    "        \"\",\n",
    "        f\"{valor_p:.6f} {nivel_sig}\",\n",
    "        f\"{tamano_efecto:.2f} ({interpretacion_efecto})\",\n",
    "        f\"{ic_inferior:.1%}\",\n",
    "        f\"{ic_superior:.1%}\",\n",
    "        \"Sí\" if ic_inferior > 0.5 else \"No\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "df_resultados = pd.DataFrame(resumen_resultados)\n",
    "print(df_resultados.to_string(index=False))\n",
    "\n",
    "# Bandas más importantes\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"BANDAS DE FRECUENCIA MÁS DISCRIMINATIVAS\")\n",
    "print(\"-\" * 70)\n",
    "print(resumen_banda.round(3).to_string())\n",
    "\n",
    "# Interpretación\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"INTERPRETACIÓN\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "Pregunta de Investigación:\n",
    "  ¿Podemos distinguir entre condiciones de audio lento y rápido en infantes\n",
    "  basándonos en la topología de conectividad EEG?\n",
    "\n",
    "Resultados:\n",
    "  - Precisión validada cruzadamente: {puntajes_cv.mean():.1%} (azar = 50%)\n",
    "  - Esto es {(puntajes_cv.mean() - 0.5) * 100:.1f} puntos porcentuales sobre el azar\n",
    "  - Valor p = {valor_p:.6f} → {\"estadísticamente significativo\" if valor_p < 0.05 else \"no estadísticamente significativo\"}\n",
    "  - Tamaño del efecto = {tamano_efecto:.2f} → significancia práctica {interpretacion_efecto}\n",
    "  - IC 95% = [{ic_inferior:.1%}, {ic_superior:.1%}]\n",
    "\"\"\")\n",
    "\n",
    "# Nivel de evidencia\n",
    "if puntajes_cv.mean() > 0.65 and valor_p < 0.05 and ic_inferior > 0.5:\n",
    "    evidencia = \"FUERTE\"\n",
    "    conclusion = \"\"\"\n",
    "Las características topológicas de los grafos de conectividad EEG distinguen exitosamente\n",
    "entre condiciones de audio lento y rápido. Esto sugiere que:\n",
    "\n",
    "1. El audio lento vs rápido induce patrones de conectividad cerebral mediblemente diferentes\n",
    "2. Estas diferencias son robustas entre sujetos (no artefactos específicos del sujeto)\n",
    "3. El Análisis de Datos Topológicos captura diferencias significativas en la señal neural\n",
    "\n",
    "Las bandas de frecuencia más discriminativas proporcionan información sobre qué tipos de\n",
    "oscilaciones neurales son más afectadas por la velocidad del audio.\"\"\"\n",
    "\n",
    "elif puntajes_cv.mean() > 0.55 and valor_p < 0.05:\n",
    "    evidencia = \"MODERADA\"\n",
    "    conclusion = \"\"\"\n",
    "Hay evidencia moderada de que las características topológicas pueden distinguir condiciones.\n",
    "La precisión está sobre el azar y es estadísticamente significativa, pero el tamaño\n",
    "del efecto es modesto. Esto sugiere:\n",
    "\n",
    "1. Existen algunas diferencias en topología de conectividad entre condiciones\n",
    "2. La señal puede ser débil o variable entre sujetos\n",
    "3. Considerar: más sujetos, diferentes características, o enfoques alternativos\"\"\"\n",
    "\n",
    "else:\n",
    "    evidencia = \"DÉBIL/NINGUNA\"\n",
    "    conclusion = \"\"\"\n",
    "Las características topológicas no distinguen confiablemente entre condiciones.\n",
    "Posibles explicaciones:\n",
    "\n",
    "1. La velocidad del audio puede no afectar significativamente la topología de conectividad EEG\n",
    "2. El efecto existe pero es demasiado sutil para los métodos actuales\n",
    "3. Se necesitan más sujetos o diferente preprocesamiento\n",
    "4. Se podrían explorar enfoques TDA alternativos (ej., diferentes filtraciones)\"\"\"\n",
    "\n",
    "print(f\"Nivel de Evidencia: {evidencia}\")\n",
    "print(conclusion)\n",
    "\n",
    "# Guardar resultados\n",
    "diccionario_resultados = {\n",
    "    \"precision_cv_media\": puntajes_cv.mean(),\n",
    "    \"precision_cv_std\": puntajes_cv.std(),\n",
    "    \"puntaje_f1_cv\": f1_cv,\n",
    "    \"valor_p\": valor_p,\n",
    "    \"tamano_efecto\": tamano_efecto,\n",
    "    \"ic_inferior\": ic_inferior,\n",
    "    \"ic_superior\": ic_superior,\n",
    "    \"n_muestras\": X.shape[0],\n",
    "    \"n_caracteristicas\": X.shape[1],\n",
    "    \"n_sujetos\": len(np.unique(sujetos)),\n",
    "    \"nivel_evidencia\": evidencia,\n",
    "}\n",
    "\n",
    "# Guardar como JSON\n",
    "import json\n",
    "with open(DIRECTORIO_RESULTADOS / \"resumen_resultados.json\", \"w\") as f:\n",
    "    json.dump(diccionario_resultados, f, indent=2)\n",
    "\n",
    "print(f\"\\nResultados guardados en {DIRECTORIO_RESULTADOS}/\")\n",
    "print(\"  - matriz_confusion_cv.png\")\n",
    "print(\"  - importancia_caracteristicas.png\")\n",
    "print(\"  - pruebas_estadisticas.png\")\n",
    "print(\"  - distribucion_sujetos.png\")\n",
    "print(\"  - diagrama_persistencia_ejemplo.png\")\n",
    "print(\"  - resumen_resultados.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ANÁLISIS COMPLETADO\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
